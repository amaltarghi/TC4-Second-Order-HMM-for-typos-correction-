{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# __Second-Order HMM for typos correction__\n",
    "_Author_ : Xiyu ZHANG and Yaohui WANG  \n",
    "_Date_   : Wed Oct 19 23:53:48 CEST 2016  \n",
    "_HomePage_ : [http://www.xiyuzhang.com](http://www.xiyuzhang.com)  \n",
    "_Email_  : zacharie.france@gmail.com\n",
    "### Introduction\n",
    "The goal is to design a model to correct typos in texts without a _dictionary_.  \n",
    "In this project, a state refers to the correct letter that should have been typed, and an observation refers to the actual letter that is typed.  \n",
    "Given a sequence of outputs/observations (i.e., actually typed letters), the problem is to reconstruct the hidden state sequence (i.e., the intended sequence of letters). \n",
    "### Data example\n",
    "Data for this problem looks like:  \n",
    "```\n",
    "[[('o', 'o'), ('f', 'f')], [('p', 'l'), ('j', 'i'), ('b', 'b'), ('e', 'e'), ('r', 'r'), ('a', 'a'), ('t', 't'), ('i', 'i'), ('o', 'o'), ('n', 'n')], [('i', 'i'), ('n', 'n')]]  \n",
    "```\n",
    "The first and third example are correctly typed.  \n",
    "The second example is misspelled: the observation is \"pjberation\" while the correct word is \"liberation\".\n",
    "### Source of Data\n",
    "Data for this problem was generated as follows: starting with a text document, in this case, the Unabomber's Manifesto, which was chosen not for political reasons, but for its convenience being available on-line and of about the right length, all numbers and punctuation were converted to white space and all letters converted to lower case.  \n",
    "The remaining text is a sequence only over the lower case letters and the space character, represented in the data files by an underscore character.  \n",
    "Next, typos were artificially added to the data as follows: with 90% probability, the correct letter is transcribed, but with 10% probability, a randomly chosen neighbor (on an ordinary physical keyboard) of the letter is transcribed instead. Space characters are always transcribed correctly. In a harder variant of the problem, the rate of errors is increased to 20%.  \n",
    "This corpus contains 4 pickles: __train10__ and __test10__ constitute the dataset with __10%__ or spelling errors, while __train20__ and __test20__ the one with __20%__ or errors.  \n",
    "### Experiment\n",
    "To improve the performance, we can increase the order of the HMM. Implement a second order model for this task is better solution than first order Hmm from our points of view.\n",
    "As for HMM2, the probability of the next state depends on the current state and the previous one as well. Besides, the current obervation depends only current state. A convenient way to implement a second order HMM, is to think about it as a variable change.  \n",
    "After, we compute the error rate (at the character level) and compare this results with the dummiest classifier that just do nothing. Meanwhile we also compute the number of errors your model can correct and the number of errors your model creates.\n",
    "### Conclusions\n",
    "This project describes an extension to the hidden first order Markov model for correcting typos. The result shows us the improvement of performance comes from using second order approximation of the Markov assumptions.\n",
    "However our model only deals with substitution errors. In the future, we will extend this model to also handle noisy insertion of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "import cPickle\n",
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary letters (OOVs)\n",
    "UNKid = 0      # index for UNK\n",
    "epsilon = 1e-100\n",
    "class HMM_2:\n",
    "    def __init__(self, state_list, observation_list,\n",
    "                 transition_proba=None,transition_head_proba=None,\n",
    "                 observation_proba=None,\n",
    "                 initial_state_proba=None, smoothing=0.01):\n",
    "        \"\"\"\n",
    "        Builds a Hidden 2 order Markov Model\n",
    "        * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "        * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "        * transition_proba is the transition probability matrix, the size is N * (N*N)\n",
    "            [a_kji] a_[k, j*N+i] = Pr(Y_(t) = q_k | Y_(t-1) = q_j and Y_(t-2) = q_i)\n",
    "        * transition_head_proba is transition probability matrix, however it is only the transition bettween the first state and second one.\n",
    "            [a_fkj] a_[k, j] = Pr(Y_(t) = q_k | Y_(t-1) = q_j)\n",
    "        * observation_proba is the observation probablility matrix\n",
    "            [b_mk] b_mk = Pr(X_t = v_m | Y_t = q_k)\n",
    "        * initial_state_proba is the initial state distribution\n",
    "            [p_i] p_i = Pr(Y_0 = q_i)\n",
    "        * attention: We use inverse row and col rather than example in book because `/` represent divide by rows in numpy.\"\"\"\n",
    "        print \"HMM_2 creating with: \"\n",
    "        self.N = len(state_list)       # number of states\n",
    "        self.M = len(observation_list)  # number of possible emissions\n",
    "        print str(self.N) + \" states\"\n",
    "        print str(self.M) + \" observations\"\n",
    "        self.omega_Y = [s for s in state_list]\n",
    "        self.omega_X = [o for o in observation_list]\n",
    "        if transition_proba is None:\n",
    "            self.transition_proba = zeros((self.N, self.N**2), dtype = np.float)\n",
    "        else:\n",
    "            self.transition_proba = transition_proba\n",
    "        if transition_head_proba is None:\n",
    "            self.transition_head_proba = zeros((self.N, self.N), dtype = np.float)\n",
    "        else:\n",
    "            self.transition_head_proba = transition_head_proba\n",
    "        if observation_proba is None:\n",
    "            self.observation_proba = zeros((self.M, self.N), dtype = np.float)\n",
    "        else:\n",
    "            self.observation_proba = observation_proba\n",
    "        if initial_state_proba is None:\n",
    "            self.initial_state_proba = zeros(self.N, dtype = np.float)\n",
    "        else:\n",
    "            self.initial_state_proba = initial_state_proba\n",
    "        self.make_indexes()  # build indexes, i.e the mapping between token and int\n",
    "        self.smoothing = smoothing\n",
    "    def make_indexes(self):\n",
    "        \"\"\"Creates the reverse table that maps states/observations names\n",
    "        to their index in the probabilities array\"\"\"\n",
    "        self.Y_index = {}\n",
    "        for i in range(self.N):\n",
    "            self.Y_index[self.omega_Y[i]] = i\n",
    "        self.X_index = {}\n",
    "        for i in range(self.M):\n",
    "            self.X_index[self.omega_X[i]] = i\n",
    "        print \"Y_index:\\n\", self.Y_index\n",
    "        print \"X_index:\\n\", self.X_index\n",
    "    def get_observationIndices(self, observations):\n",
    "        \"\"\"return observation indices, i.e\n",
    "        return [self.O_index[o] for o in observations]\n",
    "        and deals with OOVs\n",
    "        \"\"\"\n",
    "        indices = zeros(len(observations), int)\n",
    "        k = 0\n",
    "        for o in observations:\n",
    "            if o in self.X_index:\n",
    "                indices[k] = self.X_index[o]\n",
    "            else:\n",
    "                indices[k] = UNKid\n",
    "            k += 1\n",
    "        return indices\n",
    "    def data2indices(self, word):\n",
    "        \"\"\"From one tagged word of the given corpus:\n",
    "        - extract the letters and tags\n",
    "        - returns two list of indices, one for each\n",
    "        -> (letterids, tagids)\n",
    "        \"\"\"\n",
    "        letterids = list()\n",
    "        tagids = list()\n",
    "        for couple in word:\n",
    "            letter = couple[0]\n",
    "            tag = couple[1]\n",
    "            if letter in self.X_index:\n",
    "                letterids.append(self.X_index[letter])\n",
    "            else:\n",
    "                letterids.append(UNKid)\n",
    "            tagids.append(self.Y_index[tag])\n",
    "        return letterids, tagids\n",
    "    def observation_estimation(self, pair_counts):\n",
    "        \"\"\" Build the observation distribution:\n",
    "            observation_proba is the observation probablility matrix, k is current state and m is current obervation.\n",
    "            b[m, k] = Pr(X_t=v_m|Y_t=q_k)\"\"\"\n",
    "        # fill with counts\n",
    "        for pair in pair_counts:\n",
    "            letter = pair[0]\n",
    "            tag = pair[1]\n",
    "            cpt = pair_counts[pair]\n",
    "            k = 0  # for <unk>\n",
    "            if letter in self.X_index:\n",
    "                k = self.X_index[letter]\n",
    "            j = self.Y_index[tag]\n",
    "            self.observation_proba[k, j] = cpt\n",
    "        # normalize, smoothing est pour eviter prob(x(t), s(i)) == 0.\n",
    "        # Apres, on devra faire une normalisation quand meme.\n",
    "        self.observation_proba = self.observation_proba + self.smoothing\n",
    "        # smoothing > 0 et apartient Real. il n'est pas obgalitoire de etre inferieur que 1.\n",
    "        self.observation_proba = self.observation_proba / \\\n",
    "            self.observation_proba.sum(axis=0).reshape(1, self.N)\n",
    "    def transition_estimation(self, trans_counts):\n",
    "        \"\"\" Build the transition distribution:\n",
    "            transition_proba is the transition matrix with :\n",
    "            a[k,(i*N+j)] = Pr(Y_(t)=q_k|Y_(t-1)=q_j and Y_(t-2)=q_i)\n",
    "        \"\"\"\n",
    "        # fill with counts\n",
    "        for pair in trans_counts:\n",
    "            k = self.Y_index[pair[1]]\n",
    "            #((q_i, q_j), q_k) <==> ((Y_(t-2), Y_(t-1)), Y_(t))\n",
    "            i = self.Y_index[pair[0][0]]\n",
    "            j = self.Y_index[pair[0][1]]\n",
    "            self.transition_proba[k, (i * self.N + j)] = trans_counts[pair]\n",
    "            # normalize\n",
    "        self.transition_proba = self.transition_proba + self.smoothing\n",
    "        self.transition_proba = self.transition_proba / \\\n",
    "            self.transition_proba.sum(axis=0).reshape(1, self.N**2)\n",
    "    def transition_head_estimation(self, trans_heads_counts):\n",
    "        \"\"\" Build the transition distribution:\n",
    "            transition_proba is the transition matrix with :\n",
    "            a[k,j] = Pr(Y_(t)=q_k|Y_(t-1)=q_j)\n",
    "        \"\"\"\n",
    "        # fill with counts\n",
    "        for pair in trans_heads_counts:\n",
    "            j = self.Y_index[pair[0]]\n",
    "            k = self.Y_index[pair[1]]\n",
    "            self.transition_head_proba[k, j] = trans_heads_counts[pair]\n",
    "        # normalize\n",
    "        self.transition_head_proba = self.transition_head_proba + self.smoothing\n",
    "        self.transition_head_proba = self.transition_head_proba / \\\n",
    "            self.transition_head_proba.sum(axis=0).reshape(1, self.N)\n",
    "\n",
    "    def init_estimation(self, init_counts):\n",
    "        \"\"\"Build the initial distribution.\n",
    "        The initial distribution μ of Y0 on S, such that,\n",
    "        for every states x in S, one has P(Y0=x)=μ(x)\"\"\"\n",
    "        # fill with counts\n",
    "        for tag in init_counts:\n",
    "            k = self.Y_index[tag]\n",
    "            self.initial_state_proba[k] = init_counts[tag]\n",
    "        # normalize\n",
    "        self.initial_state_proba = self.initial_state_proba / \\\n",
    "            sum(self.initial_state_proba)\n",
    "\n",
    "    def supervised_training(self, pair_counts, trans_counts, trans_heads_counts, init_counts):\n",
    "        \"\"\" Train the HMM_2's parameters. This function wraps everything\"\"\"\n",
    "        self.observation_estimation(pair_counts)\n",
    "        self.transition_estimation(trans_counts)\n",
    "        self.transition_head_estimation(trans_heads_counts)\n",
    "        self.init_estimation(init_counts)\n",
    "\n",
    "    def viterbi(self, obs):\n",
    "        \"\"\"Viterbi algorithm:\n",
    "        Find the states corresponding to the oberservations.\n",
    "        @obs:The oberservations must be converted in a list of indices.\n",
    "        \"\"\"\n",
    "        # shortcuts about this class's functions\n",
    "        B = self.observation_proba\n",
    "        A = self.transition_proba\n",
    "        A_head = self.transition_head_proba\n",
    "        T = len(obs)\n",
    "        N = self.N\n",
    "        # initialisation\n",
    "        delta = zeros(N, float)\n",
    "        psi = zeros((T, N), int)\n",
    "        delta_t = zeros(N, float)\n",
    "        # apply initial_state probs to the first frame\n",
    "        delta =  self.initial_state_proba * B[obs[0]]\n",
    "        # recursion\n",
    "        for t in range(1, T):\n",
    "            O_t = obs[t]\n",
    "            if t == 1:\n",
    "                # we start to consider the second oberservation\n",
    "                for k in range(N):\n",
    "                    # loop state of the second oberservation, one by one\n",
    "                    tmp = zeros(N, float)\n",
    "                    # tmp contains the probabilty of different first state when current state is k\n",
    "                    for j in range(N):\n",
    "                        tmp[j] = delta[j] * A_head[k, j]\n",
    "                    # psi[t, k] means who the first state is such that the probabilty is maximal when second state is k.\n",
    "                    psi[t, k] = tmp.argmax()\n",
    "                    # we will use delta_t to update delta later.\n",
    "                    delta_t[k] = tmp.max() * B[O_t, k]\n",
    "            else:\n",
    "                # now we consider the third oberservation\n",
    "                for k in range(N):\n",
    "                    # loop the current state one by one\n",
    "                    tmp = zeros(N, float)\n",
    "                    for j in range(N):\n",
    "                        # loop the previous state one by one\n",
    "                        tmp[j] = delta[j] * A[k, psi[t-1, j] * N + j]\n",
    "                        # Once the previous is fixed by j, then we can infer the preprevious state by psi matrix.\n",
    "                        # psi[t-1, j] means who preprevious state is when the previous oberservation is t-1 and j is previous state.\n",
    "                    # stock the previous state such that the proba is maximal.\n",
    "                    psi[t, k] = tmp.argmax()\n",
    "                    delta_t[k] = tmp.max() * B[O_t, k]\n",
    "            delta, delta_t = delta_t, delta\n",
    "            # update delta by delta_t.\n",
    "        # reconstruction all states.\n",
    "        k = delta.argmax()\n",
    "        # construct the list of tags by matrix psi\n",
    "        i_star = [k]\n",
    "        # we loop oberservation one by one from tail to head.\n",
    "        for psi_o in psi[-1:0:-1]:\n",
    "            # we stop until the second oberservation\n",
    "            i_star.append(psi_o[i_star[-1]])\n",
    "        i_star.reverse()\n",
    "        return i_star\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Compute what Hmm needs\n",
    "- The state transition probability distribution\n",
    "- The obervation symbol probability distribution\n",
    "- The initial state distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\"\n",
    "    Build different count tables to train a HMM_2. Each count table is a dictionnary.\n",
    "    Returns:\n",
    "    * c_letters: letters counts\n",
    "    * c_tags: tag counts\n",
    "    * c_pairs: count of pairs (pre_tag, tag, letter)\n",
    "    * c_transitions: count of tag 3-gram\n",
    "    * c_inits: count of 2-gram found in the first and second position\n",
    "    \"\"\"\n",
    "    c_letters = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs = dict()\n",
    "    c_transitions = dict()\n",
    "    c_transitions_heads = dict()\n",
    "    c_inits = dict()\n",
    "    for word in corpus:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(word)):\n",
    "            couple = word[i]\n",
    "            letter = couple[0]\n",
    "            tag = couple[1]\n",
    "            # word counts\n",
    "            if letter in c_letters:\n",
    "                c_letters[letter] = c_letters[letter] + 1\n",
    "            else:\n",
    "                c_letters[letter] = 1\n",
    "            # tag counts\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag] = c_tags[tag] + 1\n",
    "            else:\n",
    "                c_tags[tag] = 1\n",
    "            if i >= 2:\n",
    "                # transition counts, z is combination of two previous states\n",
    "                z = (word[i - 2][1], word[i - 1][1])\n",
    "                trans = (z, tag)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans] = c_transitions[trans] + 1\n",
    "                else:\n",
    "                    c_transitions[trans] = 1\n",
    "            if i == 1:\n",
    "                z = (word[i-1][1], tag)\n",
    "                if z in c_transitions_heads:\n",
    "                    c_transitions_heads[z] = c_transitions_heads[z] + 1\n",
    "                else:\n",
    "                    c_transitions_heads[z] = 1\n",
    "            if i == 0:\n",
    "                # init counts, i == 1 -> counts for initial states\n",
    "                z = tag\n",
    "                if z in c_inits:\n",
    "                    c_inits[z] = c_inits[z] + 1\n",
    "                else:\n",
    "                    c_inits[z] = 1\n",
    "            # observation counts\n",
    "            o = (letter, tag)\n",
    "            if o in c_pairs:\n",
    "                c_pairs[o] = c_pairs[o] + 1\n",
    "            else:\n",
    "                c_pairs[o] = 1\n",
    "\n",
    "    return c_letters, c_tags, c_pairs, c_transitions, c_transitions_heads, c_inits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Creation of vocabulary according to the number of occurence for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_vocab(c_letters, threshold):\n",
    "    \"\"\"\n",
    "    return a vocabulary by thresholding letter counts.\n",
    "    inputs:\n",
    "    * c_letters : a dictionnary that maps letter to its counts\n",
    "    * threshold: count must be >= to the threshold to be included\n",
    "    returns:\n",
    "    * a letter list\n",
    "    \"\"\"\n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    for l in c_letters:\n",
    "        if c_letters[l] >= threshold:\n",
    "            voc.append(l)\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Loading two data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-5-992009458872>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-992009458872>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    f_in = open(\"'/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /test10.pkl', \"r\")\u001b[0m\n\u001b[0m                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# read train data\n",
    "f_in = open(\"'/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /train10.pkl'\", \"r\")\n",
    "data_train10 = cPickle.load(f_in)\n",
    "f_in.close()\n",
    "# read test data\n",
    "f_in = open(\"'/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /t10.pkl'\", \"r\")\n",
    "\n",
    "\n",
    "data_test10 = cPickle.load(f_in)\n",
    "f_in.close()\n",
    "\n",
    "cletters, ctags, cpairs, ctrans,ctrans_heads, cinits = make_counts(data_train10)\n",
    "# print \"Nombre de letters  : \" + str(len(cletters))\n",
    "# print \"Nombre de tags  : \" + str(len(ctags))\n",
    "# print \"Nombre de paires: \" + str(len(cpairs))\n",
    "# print \"Nombre de trans : \" + str(len(ctrans)) + \" / \" + str(26 * 26 * 26)\n",
    "# print \"Nombre de init. : \" + str(len(cinits))\n",
    "\n",
    "vocab = make_vocab(cletters, 1)\n",
    "vocab.sort()\n",
    "print \"----------%d vocabulaire----------\" % len(vocab)\n",
    "print vocab\n",
    "\n",
    "states = ctags.keys()\n",
    "states.sort()\n",
    "print \"----------%d states----------\" % len(states)\n",
    "print states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Creation of HMM_2 and generation of three neccessary probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM_2 creating with: \n",
      "26 states\n",
      "27 observations\n",
      "Y_index:\n",
      "{'a': 0, 'c': 2, 'b': 1, 'e': 4, 'd': 3, 'g': 6, 'f': 5, 'i': 8, 'h': 7, 'k': 10, 'j': 9, 'm': 12, 'l': 11, 'o': 14, 'n': 13, 'q': 16, 'p': 15, 's': 18, 'r': 17, 'u': 20, 't': 19, 'w': 22, 'v': 21, 'y': 24, 'x': 23, 'z': 25}\n",
      "X_index:\n",
      "{'a': 1, 'c': 3, 'b': 2, 'e': 5, 'd': 4, 'g': 7, 'f': 6, 'i': 9, 'h': 8, 'k': 11, 'j': 10, 'm': 13, 'l': 12, 'o': 15, 'n': 14, 'q': 17, 'p': 16, 's': 19, 'r': 18, 'u': 21, 't': 20, 'w': 23, 'v': 22, 'y': 25, 'x': 24, 'z': 26, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM_2(state_list=states, observation_list=vocab,\n",
    "            transition_proba=None,\n",
    "            transition_head_proba=None,\n",
    "            observation_proba=None,\n",
    "            initial_state_proba=None)\n",
    "\n",
    "hmm.supervised_training(cpairs, ctrans, ctrans_heads, cinits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training data one time(we can get three neccessary matrix once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy(%) : 6983.0 / 7320.0 -> 95.3961748634\n"
     ]
    }
   ],
   "source": [
    "#data_test10=data_test10[:100]\n",
    "tot=0.0\n",
    "correct=0.0\n",
    "for word in data_test10:\n",
    "    letter_index, tag_index = hmm.data2indices(word)\n",
    "    pre_tag_index = hmm.viterbi(letter_index)\n",
    "    # print \"-----------------------------------\"\n",
    "    # print word\n",
    "    # print \"letter_index:\",letter_index\n",
    "    # print \"tag_index:\",tag_index\n",
    "    # print \"pre_tag_index\",pre_tag_index\n",
    "    # print \"-----------------------------------\"\n",
    "    correct += np.count_nonzero(np.array(tag_index) == np.array(pre_tag_index))\n",
    "    tot+=len(word)\n",
    "print \"The accuracy(%) : \"+str(correct)+\" / \"+str(tot)+ \" -> \"+ str(correct*100/tot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "HMM2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
