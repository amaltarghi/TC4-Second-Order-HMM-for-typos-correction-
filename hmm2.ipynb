{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal is to design an hidden markov second order model to correct typos in texts without a dictionary.\n",
    "\n",
    "Participants: TARGHI Amal et MOHAMED Mohamed\n",
    "\n",
    "\n",
    "targhiamal@gmail.com , mohmed.abdelkhaleq@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question2: \n",
    "Second Order HMM: To improve the performance, we can increase the order of the HMM. Implement a second Order model for this task (this means that the probability of the next state depends on the current state and the previous one as well). A convenient way to implement a second order HMM, is to think about it as a variable\n",
    "change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "import pprint, pickle\n",
    "\n",
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary words (OOVs)\n",
    "UNKid = 0      # index for UNK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data\n",
    "\n",
    "train10 =   pickle.load(open('/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /train10.pkl', 'rb'))\n",
    "\n",
    "train20 =   pickle.load(open('/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /train20.pkl', 'rb'))\n",
    "\n",
    "test10 =  pickle.load(open('/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /test10.pkl', 'rb'))\n",
    "\n",
    "test20 =  pickle.load(open('/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /test20.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HMM2:\n",
    "        def __init__(self, state_list, observation_list,transition_proba = None,transition_head_proba=None,observation_proba = None,initial_state_proba = None, smoothing_obs = 0.01):\n",
    "         \n",
    "            \"\"\"\n",
    "            Builds a Hidden Markov Model odrer 2\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix \n",
    "            [a_kji] = Pr(Y_t = q_k | Y_t-1 = q_j , Y_t-2 = q_i)\n",
    "            * transition_head_proba is transition probability matrix(transition between the first and the second state)\n",
    "            [a_kj] = Pr(Y_t = q_k | Y_t-1 = q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            \n",
    "            print \"HMM creating with: \"\n",
    "            \n",
    "            self.N = len(state_list)       # number of states\n",
    "            self.M = len(observation_list) # number of possible emissions\n",
    "            print str(self.N)+\" states\"\n",
    "            print str(self.M)+\" observations\"\n",
    "            self.omega_Y = state_list\n",
    "            self.omega_X = observation_list\n",
    "            \n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros((self.N, self.N**2), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "                \n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "                \n",
    "                \n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "                \n",
    "            if transition_head_proba is None:\n",
    "                self.transition_head_proba = zeros((self.N, self.N), float)\n",
    "            else:\n",
    "                self.transition_head_proba = transition_head_proba\n",
    "                \n",
    "            self.make_indexes() # build indexes, i.e the mapping between token and int\n",
    "            self.smoothing_obs = smoothing_obs \n",
    " \n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "            # the indice of each word in state and observation for facilite the     \n",
    "            print \"States:\\n\", self.Y_index\n",
    "            print \"Observations:\\n\", self.X_index\n",
    "        #cobs,cstat,cpairs,ctrans,cinits\n",
    "        #def get_observationIndices( self, cobs ):\n",
    "\n",
    "        def get_observationIndices( self, observations ):\n",
    "            \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs\n",
    "            \"\"\"\n",
    "            indices = zeros( len(observations), int )\n",
    "            k = 0\n",
    "            for o in observations:\n",
    "                if o in self.X_index:\n",
    "                    indices[k] = self.X_index[o]\n",
    "                else:\n",
    "                    indices[k] = UNKid\n",
    "                k += 1\n",
    "            return indices\n",
    "        \n",
    "        \"\"\"From one each word\n",
    "        - extract the letters and correction of rache one\n",
    "        - (letterObservation,LetterState)\n",
    "        \"\"\"\n",
    "        def data2indices(self, word): \n",
    "    \n",
    "            letterObservation = list()\n",
    "            letterState  = list()\n",
    "            for letter in word:\n",
    "                observation = letter[0]\n",
    "                state = letter[1]\n",
    "                if observation in self.X_index:\n",
    "                    letterObservation.append(self.X_index[observation])\n",
    "                else:\n",
    "                    letterObservation.append(UNKid)\n",
    "                letterState.append(self.Y_index[state])\n",
    "            return letterObservation,letterState\n",
    "        \n",
    "        def observation_estimation(self, cpairs):\n",
    "            \"\"\" Build the observation distribution: \n",
    "                observation_proba is the observation probablility matrix\n",
    "                [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\"\"\"\n",
    "         \n",
    "            \n",
    "            # fill with counts\n",
    "            for pair in cpairs:\n",
    "                \n",
    "                observation=pair[0]\n",
    "                state=pair[1]\n",
    "                cpt=cpairs[pair]\n",
    "                k = 0 # for <unk>\n",
    "                if observation in self.X_index: \n",
    "                    k=self.X_index[observation]\n",
    "                j=self.Y_index[state]\n",
    "                self.observation_proba[k,j]=cpt\n",
    "            # normalize\n",
    "            self.observation_proba=self.observation_proba+self.smoothing_obs\n",
    "            self.observation_proba=self.observation_proba/self.observation_proba.sum(axis=0).reshape(1,self.N)\n",
    "        \n",
    "        def transition_estimation(self, ctrans):\n",
    "            \"\"\" Build the transition distribution: \n",
    "                transition_proba is the transition matrix with : \n",
    "                k is current state\n",
    "            \"\"\"\n",
    "            # fill with counts\n",
    "            for pair in ctrans:\n",
    "                i = self.Y_index[pair[0][0]]\n",
    "                j = self.Y_index[pair[0][1]]\n",
    "                k = self.Y_index[pair[1]]\n",
    "                self.transition_proba[k, (i * self.N + j)] = ctrans[pair]\n",
    "            # normalize\n",
    "            self.transition_proba = self.transition_proba + self.smoothing_obs\n",
    "            self.transition_proba=self.transition_proba/self.transition_proba.sum(axis=0).reshape(1,self.N**2)\n",
    "     \n",
    "            \n",
    "        def init_estimation(self, cinits):\n",
    "            \"\"\"Build the init. distribution\"\"\"\n",
    "            # fill with counts\n",
    "            for state in cinits:\n",
    "                i=self.Y_index[state]\n",
    "                self.initial_state_proba[i]=cinits[state]\n",
    "            # normalize\n",
    "            self.initial_state_proba=self.initial_state_proba/sum(self.initial_state_proba)\n",
    "            \n",
    "        def transition_head_estimation(self, trans_heads_counts):\n",
    "       \n",
    "        # fill with counts\n",
    "            for pair in trans_heads_counts:\n",
    "                j = self.Y_index[pair[0]]\n",
    "                k = self.Y_index[pair[1]]\n",
    "                self.transition_head_proba[k, j] = trans_heads_counts[pair]\n",
    "        # normalize\n",
    "            self.transition_head_proba = self.transition_head_proba + self.smoothing_obs\n",
    "            self.transition_head_proba = self.transition_head_proba / self.transition_head_proba.sum(axis=0).reshape(1, self.N)\n",
    "\n",
    "        \n",
    "        def supervised_training(self, cpairs, ctrans,ctrans_head,cinits):\n",
    "            \"\"\" Train the HMM's parameters. This function wraps everything\"\"\"\n",
    "            self.observation_estimation(cpairs)\n",
    "            self.transition_estimation(ctrans)\n",
    "            self.transition_head_estimation(ctrans_head)\n",
    "            self.init_estimation(cinits)\n",
    "           \n",
    "            # Viterbi Algortihm\n",
    "            # Source: http://stackoverflow.com/questions/34219766/need-help-implementing-viterbi-for-a-second-order-hidden-markov-model-with-pytho\n",
    "            \n",
    "        def viterbi( self, obs ):\n",
    "            \"\"\"Find the most probable state sequence\n",
    "            \"\"\"\n",
    "\n",
    "            # shortcuts\n",
    "            B = self.observation_proba \n",
    "            A = self.transition_proba\n",
    "            A_head = self.transition_head_proba\n",
    "            T = len(obsids)\n",
    "            N = self.N\n",
    "            # initialisation\n",
    "            delta = zeros( N, float )\n",
    "            tmp = zeros(N, float)\n",
    "            psi = zeros( (T, N), int )\n",
    "            #updated delta\n",
    "            delta_t = zeros( N, float )\n",
    "            # apply initial_state probs to the first frame\n",
    "            delta = B[obsids[0]] * self.initial_state_proba   \n",
    "            # recursion\n",
    "            for t in xrange(1, T):\n",
    "                O_t = obsids[t]\n",
    "                if t == 1:\n",
    "                    # i current state\n",
    "                    #the second oberservation \n",
    "                    #tmp proba of of different diff state when i\n",
    "                    for i in range(N):\n",
    "                        for j in range(N):\n",
    "                            tmp[j] = delta[j] * A_head[i, j]\n",
    "                        psi[t, i] = tmp.argmax()\n",
    "                        delta_t[i] = tmp.max() * B[O_t, i]\n",
    "                #third obs\n",
    "                else:\n",
    "                    for i in range (N):\n",
    "                        for j in range(N):\n",
    "                            tmp[j] = delta[j] * A[i, psi[t-1, j] * N + j]\n",
    "                        psi[t, i] = tmp.argmax()\n",
    "                        delta_t[i] = tmp.max() * B[O_t, i]\n",
    "                delta, delta_t = delta_t, delta\n",
    "                \n",
    "            # reconstruction\n",
    "            i_star = [delta.argmax()]                        \n",
    "            for psi_t in psi[-1:0:-1]:\n",
    "                i_star.append( psi_t[i_star[-1]] )                 \n",
    "            i_star.reverse()\n",
    "            return i_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compter les lettres et les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\"\n",
    "    Build different count tables to train a HMM_2. Each count table is a dictionnary.\n",
    "    Returns:\n",
    "    * c_letterObservation: No correct Letter Count (Observation Letter)\n",
    "    * c_letterState:  Correct Letters  ( State Letters)\n",
    "    * c_pairs: count of pairs \n",
    "    * c_transitions: count of tag 3-gram\n",
    "    * c_inits: count of 2-gram found in the first and second position\n",
    "    \"\"\"\n",
    "    c_letterObservation = dict()\n",
    "    c_letterState = dict()\n",
    "    c_pairs = dict()\n",
    "    c_transitions = dict()\n",
    "    ctrans_head = dict()\n",
    "    c_inits = dict()\n",
    "    for word in corpus:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(word)):\n",
    "            couple = word[i]\n",
    "            letter = couple[0]\n",
    "            tag = couple[1]\n",
    "            # word counts\n",
    "            if letter in c_letterObservation:\n",
    "                c_letterObservation[letter] = c_letterObservation[letter] + 1\n",
    "            else:\n",
    "                c_letterObservation[letter] = 1\n",
    "            # tag counts\n",
    "            if tag in c_letterState:\n",
    "                c_letterState[tag] = c_letterState[tag] + 1\n",
    "            else:\n",
    "                c_letterState[tag] = 1\n",
    "            if i >= 2:\n",
    "                # transition counts, z is combination of two previous states\n",
    "                z = (word[i - 2][1], word[i - 1][1])\n",
    "                trans = (z, tag)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans] = c_transitions[trans] + 1\n",
    "                else:\n",
    "                    c_transitions[trans] = 1\n",
    "            if i == 1:\n",
    "                z = (word[i-1][1], tag)\n",
    "                if z in ctrans_head:\n",
    "                    ctrans_head[z] = ctrans_head[z] + 1\n",
    "                else:\n",
    "                    ctrans_head[z] = 1\n",
    "            if i == 0:\n",
    "                # init counts, i == 1 -> counts for initial states\n",
    "                z = tag\n",
    "                if z in c_inits:\n",
    "                    c_inits[z] = c_inits[z] + 1\n",
    "                else:\n",
    "                    c_inits[z] = 1\n",
    "            # observation counts\n",
    "            o = (letter, tag)\n",
    "            if o in c_pairs:\n",
    "                c_pairs[o] = c_pairs[o] + 1\n",
    "            else:\n",
    "                c_pairs[o] = 1\n",
    "\n",
    "    return c_letterObservation, c_letterState, c_pairs, c_transitions, ctrans_head, c_inits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of vocabulary according to the number of occurence for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(c_letterObservation, threshold):\n",
    "    \"\"\" \n",
    "    return a vocabulary by thresholding word counts. \n",
    "    inputs: \n",
    "    * c_words : a dictionnary that maps word to its counts\n",
    "    * threshold: count must be >= to the threshold to be included\n",
    "    \n",
    "    returns: \n",
    "    * a word list\n",
    "    \"\"\"\n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    for letter in c_letterObservation:\n",
    "        if c_letterObservation[letter] >= threshold:\n",
    "            voc.append(letter)\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases de train = 29057\n",
      "Nombre de phrases de test  = 1501\n",
      "Nombre de mots  : 26\n",
      "Nombre de tags  : 26\n",
      "Nombre de paires: 127\n",
      "Nombre de trans : 2489 / 676\n",
      "Nombre de init. : 25\n",
      "Vocabulaire :27\n"
     ]
    }
   ],
   "source": [
    "clettero,cletterS,cpairs,ctrans,ctrans_head, cinits = make_counts(train10)\n",
    "#i will test for train20 and train 10 (both  of them)\n",
    "print \"Nombre de phrases de train = \"+str(len(train10))\n",
    "print \"Nombre de phrases de test  = \"+str(len(test10))\n",
    "print \"Nombre de mots  : \"+str(len(clettero))\n",
    "print \"Nombre de tags  : \"+str(len(cletterS))\n",
    "print \"Nombre de paires: \"+str(len(cpairs))\n",
    "print \"Nombre de trans : \"+str(len(ctrans))+ \" / \"+ str(len(cletterS)*len(cletterS))\n",
    "print \"Nombre de init. : \"+str(len(cinits))\n",
    "vocab = make_vocab(clettero,10)\n",
    "print \"Vocabulaire :\"+str(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation du HMM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "27 observations\n",
      "States:\n",
      "{'a': 0, 'c': 1, 'b': 2, 'e': 3, 'd': 4, 'g': 5, 'f': 6, 'i': 7, 'h': 8, 'k': 9, 'j': 10, 'm': 11, 'l': 12, 'o': 13, 'n': 14, 'q': 15, 'p': 16, 's': 17, 'r': 18, 'u': 19, 't': 20, 'w': 21, 'v': 22, 'y': 23, 'x': 24, 'z': 25}\n",
      "Observations:\n",
      "{'a': 1, 'c': 2, 'b': 3, 'e': 4, 'd': 5, 'g': 6, 'f': 7, 'i': 8, 'h': 9, 'k': 10, 'j': 11, 'm': 12, 'l': 13, 'o': 14, 'n': 15, 'q': 16, 'p': 17, 's': 18, 'r': 19, 'u': 20, 't': 21, 'w': 22, 'v': 23, 'y': 24, 'x': 25, 'z': 26, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "hmm2 = HMM2(state_list=cletterS.keys(), observation_list=vocab,\n",
    "                 transition_proba = None,\n",
    "                 transition_head_proba=None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None)\n",
    "\n",
    "hmm2.supervised_training(cpairs, ctrans, ctrans_head , cinits)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propabibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmm2.observation_estimation(cpairs)\n",
    "hmm2.transition_estimation(ctrans)\n",
    "hmm2.init_estimation(cinits)\n",
    "#print hmm.observation_proba.sum(axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrected ones are 6983\n",
      "total is 7320\n",
      "Accuarcy is 95.3961748634 %\n",
      "Error is 4.60382513661 %\n"
     ]
    }
   ],
   "source": [
    "#test 10 \n",
    "correct=0\n",
    "total=0\n",
    "\n",
    "for word in test10:\n",
    "    obsids,statids = hmm2.data2indices(word)\n",
    "    best_sequence = hmm2.viterbi(obsids)\n",
    "    correct+=sum(np.array(best_sequence)==np.array(statids))\n",
    "    total+=len(statids)\n",
    "    \n",
    "    \n",
    "    #print statids\n",
    "    #print obsids\n",
    "    #print best_sequence\n",
    "    \n",
    "print('corrected ones are ' +str(correct))\n",
    "print('total is ' +str(total))\n",
    "\n",
    "print \"Accuarcy is \"+str(correct*100.0/total ) + \" %\"\n",
    "print \"Error is \"+str(100 - correct*100.0/total ) + \" %\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 9, 4]\n",
      "[20, 8, 3]\n",
      "[20, 8, 3]\n",
      "[13, 4, 7, 21, 8, 18, 21]\n",
      "[12, 3, 6, 20, 7, 17, 20]\n",
      "[12, 3, 6, 20, 7, 17, 20]\n",
      "[8, 18]\n",
      "[7, 17]\n",
      "[7, 17]\n",
      "[21, 14, 14]\n",
      "[20, 13, 13]\n",
      "[20, 13, 13]\n",
      "[7, 1, 19]\n",
      "[6, 0, 18]\n",
      "[6, 0, 18]\n",
      "[6, 14, 15, 4]\n",
      "[5, 13, 14, 3]\n",
      "[5, 13, 14, 3]\n",
      "[7, 14, 19]\n",
      "[6, 13, 18]\n",
      "[6, 13, 18]\n",
      "[21, 9, 1, 21]\n",
      "[20, 8, 0, 20]\n",
      "[20, 8, 0, 20]\n",
      "[9, 8, 18]\n",
      "[8, 7, 17]\n",
      "[8, 7, 17]\n",
      "[19, 4, 4, 13, 11, 9, 6, 18]\n",
      "[6, 3, 3, 12, 7, 14, 5, 17]\n",
      "[18, 3, 3, 12, 7, 14, 5, 17]\n",
      "[14, 7]\n",
      "[13, 6]\n",
      "[13, 6]\n",
      "[8, 15, 7, 4, 19, 8, 10, 19, 8, 6, 24]\n",
      "[7, 14, 6, 3, 18, 7, 13, 18, 7, 20, 23]\n",
      "[7, 14, 6, 3, 18, 7, 13, 18, 7, 20, 23]\n",
      "[1, 19, 4]\n",
      "[0, 18, 3]\n",
      "[0, 18, 3]\n",
      "[18, 13]\n",
      "[17, 13]\n",
      "[17, 13]\n",
      "[8, 15, 6, 19, 26, 8, 15, 4, 5]\n",
      "[7, 14, 5, 18, 0, 7, 14, 3, 4]\n",
      "[7, 14, 5, 18, 0, 7, 14, 3, 4]\n",
      "[21, 9, 1, 21]\n",
      "[20, 8, 0, 20]\n",
      "[20, 8, 0, 20]\n",
      "[9, 4]\n",
      "[8, 3]\n",
      "[8, 3]\n",
      "[2, 1, 15, 15, 14, 21]\n",
      "[1, 0, 14, 14, 13, 20]\n",
      "[1, 0, 14, 14, 13, 20]\n",
      "[2, 14, 15, 2, 4, 8, 23, 4]\n",
      "[1, 13, 14, 1, 3, 7, 22, 3]\n",
      "[1, 13, 14, 1, 3, 7, 22, 3]\n",
      "[14, 7]\n",
      "[13, 6]\n",
      "[13, 6]\n",
      "Accuarcy is 95.4379808341 %\n",
      "Error is 4.56201916588 %\n"
     ]
    }
   ],
   "source": [
    "# results for  20 first data  ( to be more clear thaaan because the data est volumineuse)\n",
    "\n",
    "    \n",
    "#obsids is the word (observation) (false)\n",
    "#statids is real word\n",
    "#best_sequence is the correction of the word \n",
    "# there are just the indices of each word\n",
    "# we can find the word by checking the result of creation of hmm\n",
    "# for example for the first case\n",
    "#[19, 4, 4, 13, 11, 9, 6, 18] = reeljhgs false word\n",
    "#[6, 3, 3, 12, 7, 14, 5, 17] =  feelings  true word\n",
    "#[18, 3, 3, 12, 7, 14, 5, 17] =  reelings after correction\n",
    "#the hmm can't correct the word on this case\n",
    "\n",
    "#[8, 15, 6, 19, 26, 8, 15, 4, 5] =  false word ingrzined\n",
    "#[7, 14, 5, 18, 0, 7, 14, 3, 4] =  ture word ingrained \n",
    "#[7, 14, 5, 18, 0, 7, 14, 3, 4] = corrected word ingrained\n",
    "\n",
    "\n",
    "from itertools import islice\n",
    "head = list(islice(test10, 20))\n",
    "#print head\n",
    "\n",
    "for word in head:\n",
    "    obsids,statids = hmm2.data2indices(word)\n",
    "    best_sequence = hmm2.viterbi(obsids)\n",
    "    correct+=sum(np.array(best_sequence)==np.array(statids))\n",
    "    total+=len(word)\n",
    "    \n",
    "    print obsids\n",
    "    print statids\n",
    "    print best_sequence\n",
    "\n",
    "\n",
    "\n",
    "print \"Accuarcy is \"+str(correct*100.0/total ) + \" %\"\n",
    "print \"Error is \"+str(100 - correct*100.0/total ) + \" %\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct ones are 15216\n",
      "total is 16691\n",
      "Accuarcy is 91.1629021628 %\n",
      "Error is 8.83709783716 %\n"
     ]
    }
   ],
   "source": [
    "#test 20 \n",
    "correct=0\n",
    "total=0\n",
    "\n",
    "for word in test20:\n",
    "    obsids,statids = hmm2.data2indices(word)\n",
    "    best_sequence = hmm2.viterbi(obsids)\n",
    "    correct+=sum(np.array(best_sequence)==np.array(statids))\n",
    "    total+=len(statids)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "print('correct ones are ' +str(correct))\n",
    "print('total is ' +str(total))\n",
    "\n",
    "print \"Accuarcy is \"+str(correct*100.0/total ) + \" %\"\n",
    "print \"Error is \"+str(100 - correct*100.0/total ) + \" %\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
