{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal is to design an hidden markov first order model to correct typos in texts without a dictionary.\n",
    "\n",
    "Participants: TARGHI Amal et MOHAMED Mohamed\n",
    "\n",
    "\n",
    "targhiamal@gmail.com , mohmed.abdelkhaleq@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "Dry run: Train a first-order HMM using the training data. This is basically what we did in\n",
    "    lab sessions for POS-tagging. Compute the error rate (at the character level) and\n",
    "    compare this results with the dummiest classifier that just do nothing.\n",
    "    You can also compute  the number of errors your model can correct and the number of errors your model creates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "import pprint, pickle\n",
    "\n",
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary words (OOVs)\n",
    "UNKid = 0      # index for UNK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data\n",
    "\n",
    "train10 =   pickle.load(open('/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /train10.pkl', 'rb'))\n",
    "\n",
    "train20 =   pickle.load(open('/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /train20.pkl', 'rb'))\n",
    "\n",
    "test10 =  pickle.load(open('/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /test10.pkl', 'rb'))\n",
    "\n",
    "test20 =  pickle.load(open('/home/amal/TC4-Second-Order-HMM-for-typos-correction-/typos-data /test20.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None, smoothing_obs = 0.01):\n",
    "            \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            print \"HMM creating with: \"\n",
    "            \n",
    "            self.N = len(state_list)       # number of states\n",
    "            self.M = len(observation_list) # number of possible emissions\n",
    "            print str(self.N)+\" states\"\n",
    "            print str(self.M)+\" observations\"\n",
    "            self.omega_Y = state_list\n",
    "            self.omega_X = observation_list\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            self.make_indexes() # build indexes, i.e the mapping between token and int\n",
    "            self.smoothing_obs = smoothing_obs \n",
    " \n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "                \n",
    "            print \"States:\\n\", self.Y_index\n",
    "            print \"Observations:\\n\", self.X_index\n",
    "        #cobs,cstat,cpairs,ctrans,cinits\n",
    "        #def get_observationIndices( self, cobs ):\n",
    "\n",
    "        def get_observationIndices( self, observations ):\n",
    "            \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs\n",
    "            \"\"\"\n",
    "            indices = zeros( len(observations), int )\n",
    "            k = 0\n",
    "            for o in observations:\n",
    "                if o in self.X_index:\n",
    "                    indices[k] = self.X_index[o]\n",
    "                else:\n",
    "                    indices[k] = UNKid\n",
    "                k += 1\n",
    "            return indices\n",
    "        \n",
    "        \"\"\"From one each word\n",
    "        - extract the letters and correction of rache one\n",
    "        - (letterObservation,LetterState)\n",
    "        \"\"\"\n",
    "        def data2indices(self, word): \n",
    "    \n",
    "            letterObservation = list()\n",
    "            letterState  = list()\n",
    "            for letter in word:\n",
    "                observation = letter[0]\n",
    "                state = letter[1]\n",
    "                if observation in self.X_index:\n",
    "                    letterObservation.append(self.X_index[observation])\n",
    "                else:\n",
    "                    letterObservation.append(UNKid)\n",
    "                letterState.append(self.Y_index[state])\n",
    "            return letterObservation,letterState\n",
    "        \n",
    "        def observation_estimation(self, cpairs):\n",
    "            \"\"\" Build the observation distribution: \n",
    "                observation_proba is the observation probablility matrix\n",
    "                [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\"\"\"\n",
    "            # fill with counts\n",
    "            for pair in cpairs: \n",
    "                observation=pair[0]\n",
    "                state=pair[1]\n",
    "                cpt=cpairs[pair]\n",
    "                k = 0 # for <unk>\n",
    "                if observation in self.X_index: \n",
    "                    k=self.X_index[observation]\n",
    "                i=self.Y_index[state]\n",
    "                self.observation_proba[k,i]=cpt\n",
    "            # normalize\n",
    "            self.observation_proba=self.observation_proba+self.smoothing_obs\n",
    "            self.observation_proba=self.observation_proba/self.observation_proba.sum(axis=0).reshape(1,self.N)\n",
    "        \n",
    "        def transition_estimation(self, ctrans):\n",
    "            \"\"\" Build the transition distribution: \n",
    "                transition_proba is the transition matrix with : \n",
    "                [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            \"\"\"\n",
    "            # fill with counts\n",
    "            for pair in ctrans:\n",
    "                i=self.Y_index[pair[0]]\n",
    "                j=self.Y_index[pair[1]]\n",
    "                self.transition_proba[i,j]=ctrans[pair]\n",
    "            # normalize\n",
    "            self.transition_proba=self.transition_proba/self.transition_proba.sum(axis=0).reshape(1,self.N)\n",
    "     \n",
    "            \n",
    "        def init_estimation(self, cinits):\n",
    "            \"\"\"Build the init. distribution\"\"\"\n",
    "            # fill with counts\n",
    "            for state in cinits:\n",
    "                i=self.Y_index[state]\n",
    "                self.initial_state_proba[i]=cinits[state]\n",
    "            # normalize\n",
    "            self.initial_state_proba=self.initial_state_proba/sum(self.initial_state_proba)\n",
    "        \n",
    "        def supervised_training(self, cpairs, ctrans,cinits):\n",
    "            \"\"\" Train the HMM's parameters. This function wraps everything\"\"\"\n",
    "            self.observation_estimation(cpairs)\n",
    "            self.transition_estimation(ctrans)\n",
    "            self.init_estimation(cinits)\n",
    "            \n",
    "            # Viterbi Algortihm\n",
    "            # Source: http://stackoverflow.com/questions/34219766/need-help-implementing-viterbi-for-a-second-order-hidden-markov-model-with-pytho\n",
    "        def viterbi( self, obs ):\n",
    "            \"\"\"Find the most probable state sequence\n",
    "            \"\"\"\n",
    "\n",
    "            # shortcuts\n",
    "            B = self.observation_proba \n",
    "            A = self.transition_proba\n",
    "            T = len(obsids)\n",
    "            N = self.N\n",
    "            # initialisation\n",
    "            delta = zeros( N, float )\n",
    "            tmp = zeros( N, float )\n",
    "            psi = zeros( (T, N), int )      \n",
    "            delta_t = zeros( N, float )\n",
    "            # apply initial_state probs to the first frame\n",
    "            delta = B[obsids[0]] * self.initial_state_proba   \n",
    "            # recursion\n",
    "            for t in xrange(1, T):\n",
    "                O_t = obsids[t]\n",
    "                for j in range(N):\n",
    "                    multiply( delta, A[:, j], tmp )\n",
    "                    idx = psi[t, j] = tmp.argmax()       \n",
    "                    delta_t[j] = tmp[idx] * B[O_t, j] \n",
    "                delta, delta_t = delta_t, delta\n",
    "            # reconstruction\n",
    "            i_star = [delta.argmax()]                        \n",
    "            for psi_t in psi[-1:0:-1]:\n",
    "                i_star.append( psi_t[i_star[-1]] )                 \n",
    "            i_star.reverse()\n",
    "            return i_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compter les lettres et les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_counts(train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build different count tables to train a HMM1. Each count table is a dictionnary.\n",
    "    Returns:\n",
    "    * c_observation: No correct Letter Count (Observation Letter)\n",
    "    * c_state: Correct Letters  ( State Letters)\n",
    "    * c_pairs: count of pairs (nncorrectedletr,correctedletr)\n",
    "    * c_transitions: count bigram\n",
    "    * c_inits: count of tag found in the first position\n",
    "    \"\"\"\n",
    "        \n",
    "    c_letterObservation = dict()\n",
    "    c_letterState = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions = dict()\n",
    "    c_inits = dict()\n",
    "    \n",
    "    \n",
    "    for word in train:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(word)):\n",
    "            couple=word[i]\n",
    "            observation = couple[0]\n",
    "            state = couple[1]\n",
    "            # letter counts\n",
    "            if observation in c_letterObservation:\n",
    "                c_letterObservation[observation]=c_letterObservation[observation]+1\n",
    "            else:\n",
    "                c_letterObservation[observation]=1\n",
    "            # letter  counts\n",
    "            if state in c_letterState:\n",
    "                c_letterState[state] = c_letterState[state]+1\n",
    "            else:\n",
    "                c_letterState[state]=1\n",
    "            # observation counts\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple]=c_pairs[couple]+1\n",
    "            else:\n",
    "                c_pairs[couple]=1\n",
    "            # i >  0 -> transition counts\n",
    "            if i > 0:\n",
    "                trans = (word[i-1][1],state)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans]=c_transitions[trans]+1\n",
    "                else:\n",
    "                    c_transitions[trans]=1\n",
    "            # i == 0 -> counts for initial states\n",
    "            else:\n",
    "                if state in c_inits:\n",
    "                    c_inits[state]=c_inits[state]+1\n",
    "                else:\n",
    "                    c_inits[state]=1\n",
    "                    \n",
    "    return c_letterObservation ,c_letterState,c_pairs, c_transitions, c_inits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of vocabulary according to the number of occurence for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vocab(c_letterObservation, threshold):\n",
    "    \"\"\" \n",
    "    return a vocabulary by thresholding word counts. \n",
    "    inputs: \n",
    "    * c_words : a dictionnary that maps word to its counts\n",
    "    * threshold: count must be >= to the threshold to be included\n",
    "    \n",
    "    returns: \n",
    "    * a word list\n",
    "    \"\"\"\n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    for letter in c_letterObservation:\n",
    "        if c_letterObservation[letter] >= threshold:\n",
    "            voc.append(letter)\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots  : 26\n",
      "Nombre de tags  : 26\n",
      "Nombre de paires: 127\n",
      "Nombre de trans : 403 / 676\n",
      "Nombre de init. : 25\n",
      "Vocabulaire :27\n"
     ]
    }
   ],
   "source": [
    "clettero,cletterS,cpairs,ctrans,cinits = make_counts(train10)\n",
    "print \"Nombre de mots  : \"+str(len(clettero))\n",
    "print \"Nombre de tags  : \"+str(len(cletterS))\n",
    "print \"Nombre de paires: \"+str(len(cpairs))\n",
    "print \"Nombre de trans : \"+str(len(ctrans))+ \" / \"+ str(len(cletterS)*len(cletterS))\n",
    "print \"Nombre de init. : \"+str(len(cinits))\n",
    "vocab = make_vocab(clettero,10)\n",
    "print \"Vocabulaire :\"+str(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation du HMM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "27 observations\n",
      "States:\n",
      "{'a': 0, 'c': 1, 'b': 2, 'e': 3, 'd': 4, 'g': 5, 'f': 6, 'i': 7, 'h': 8, 'k': 9, 'j': 10, 'm': 11, 'l': 12, 'o': 13, 'n': 14, 'q': 15, 'p': 16, 's': 17, 'r': 18, 'u': 19, 't': 20, 'w': 21, 'v': 22, 'y': 23, 'x': 24, 'z': 25}\n",
      "Observations:\n",
      "{'a': 1, 'c': 2, 'b': 3, 'e': 4, 'd': 5, 'g': 6, 'f': 7, 'i': 8, 'h': 9, 'k': 10, 'j': 11, 'm': 12, 'l': 13, 'o': 14, 'n': 15, 'q': 16, 'p': 17, 's': 18, 'r': 19, 'u': 20, 't': 21, 'w': 22, 'v': 23, 'y': 24, 'x': 25, 'z': 26, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=cletterS.keys(), observation_list=vocab,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None)\n",
    "\n",
    "hmm.supervised_training(cpairs, ctrans, cinits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propabibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmm.observation_estimation(cpairs)\n",
    "hmm.transition_estimation(ctrans)\n",
    "hmm.init_estimation(cinits)\n",
    "#print hmm.observation_proba.sum(axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct ones are 6777\n",
      "total is 7320\n",
      "Accuarcy is 92.5819672131 %\n"
     ]
    }
   ],
   "source": [
    "#test 10 \n",
    "correct=0\n",
    "total=0\n",
    "\n",
    "for word in test10:\n",
    "    obsids,statids = hmm.data2indices(word)\n",
    "    best_sequence = hmm.viterbi(obsids)\n",
    "    correct+=sum(np.array(best_sequence)==np.array(statids))\n",
    "    total+=len(statids)\n",
    "    \n",
    "    \n",
    "    #print statids\n",
    "    #print obsids\n",
    "    #print best_sequence\n",
    "    \n",
    "print('correct ones are ' +str(correct))\n",
    "print('total is ' +str(total))\n",
    "\n",
    "print \"Accuarcy is \"+str(correct*100.0/total ) + \" %\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct ones are 14433\n",
      "total is 16691\n",
      "Accuarcy is 86.4717512432 %\n"
     ]
    }
   ],
   "source": [
    "#test 20 \n",
    "correct=0\n",
    "total=0\n",
    "\n",
    "for word in test20:\n",
    "    obsids,statids = hmm.data2indices(word)\n",
    "    best_sequence = hmm.viterbi(obsids)\n",
    "    correct+=sum(np.array(best_sequence)==np.array(statids))\n",
    "    total+=len(statids)\n",
    "    \n",
    "    \n",
    "    #print statids\n",
    "    #print obsids\n",
    "    #print best_sequence\n",
    "    \n",
    "print('correct ones are ' +str(correct))\n",
    "print('total is ' +str(total))\n",
    "\n",
    "print \"Accuarcy is \"+str(correct*100.0/total ) + \" %\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
